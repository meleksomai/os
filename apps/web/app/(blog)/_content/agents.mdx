---
title: Agent-First Systems
subtitle: Redefining Software in the Age of AI Agents
featured: true
publishedAt: 2025-12-27
category: engineering
---

import { Callout } from "@workspace/ui/blocks/callout";
import { Quote } from "@workspace/ui/blocks/quote";
import { ThemeImage } from "@workspace/ui/blocks/themed-image";
import { GitHubIcon } from "@workspace/ui/components/icons";
import { Highlight } from "@workspace/ui/blocks/highlight";

<Callout type="info">
Over the holidays, I built an AI Assistant using Cloudflare Durable Objects. The source code is available <GitHubIcon className="inline-block w-4 h-4" /> [meleksomai/os](https://github.com/meleksomai/os) and you can try it by sending an email to [hello@somai.me](mailto:hello@somai.me). Building on that experience, this essay explores how agent-first systems are not merely a new feature layer atop existing tools, but a complete shift in the way we design and interact with systems.
</Callout>

Over the past few months, and more precisely following the release of [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) and [Gemini 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro), I have become increasingly convinced that the future of software lies in building *agent-first* systems.[^1] By these systems, I mean software in which the task to be performed *by* an end-user is further removed *from* the end-user. As current AI agents empowered by LLMs are increasingly capable of long-context reasoning, tool use, and multi-turn interaction, the tasks they can handle are becoming progressively _-and then suddenly-_ the property of the agent. Agents manage the intent interpretation, memory, decision-making, action execution, and ultimately *removes* further the user from the task itself. The shift could be thought of as analogous to the way modern high-level programming languages such as Python, JavaScript, and others abstract away garbage collection, memory allocation, etc. However, the similarity ends here.

[^1]: This is making the assumption that Frontier LLMs will continue to improve in capability, reliability, and safety over time; and that whether we achieve AGI or not, these models will be powerful enough to handle a wide range of tasks that we currently rely on traditional software for.

The magnitude of this paradigm shift is revealing itself through the radical change in the interfaces through which we as human interact with machines. The first inflection[^2] point and glimpse toward agent-based software is without doubt [ChatGPT](https://chatgpt.com/). The conversational interface made it easy to express intent, iterate on ideas, and experience the capabilities of the LLM model. The second and more powerful _Aha!_ moment came more recently when I started using AI-assisted coding with [Codex](https://openai.com/codex/), [Claude Code](https://github.com/anthropics/claude-code), and [Amp](https://ampcode.com/). All these coding tools use a simple Command-line Interface (CLI).[^3] The simplicity of the CLI unlocks instinctively a more natural and frictionless way to bring LLMs to my day-to-day programming routine. Those CLI-native tools surpass the more complex IDE-based tools such as [VS Code Copilot](https://code.visualstudio.com/docs/copilot/overview), [Cursor](https://cursor.com/) or [Zed](https://zed.dev/) that I have tried in the past months. This evolving preference for simpler, more direct interfaces is not coincidental. And it is not merely constrained to programming. Across industries, the current interfaces of modern software—from spreadsheets to dashboards to <Highlight text="Electronic Health Records are the health care equivalent of ERPs: clunky, transaction-centric systems optimized for compliance and billing rather than human workflows, powerful in their ability to enforce standardization at scale, yet ill-suited for adaptation, real-time reasoning, or the nuanced, longitudinal nature of clinical care.">EHRs</Highlight>—- are often complex, cumbersome, and frustrating to use — so much so - that the end user such as emergency department physicians are spending more time interacting with the software than performing their core work, including delivering direct patient care.[^4] The end user has to navigate menus, click on buttons, and fill out forms. The interfaces are designed to keep humans as the primary agents of understanding and action, with machines serving as fast, precise, but ultimately inert objects.[^5] They are perfected to guide and structure human reasoning, not replace it since the thought was that machine can't _reason_.

[^2]: While I describe LLM progress in terms of a step function, the underlying improvements are smoother and predictable. What feels like a sudden capability jump is often a perceptual bias shaped by our limited human capacity to measure change over a continuous spectrum. I highly recommend the following paper: <br /> _Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo._ 2023. “Are Emergent Abilities of Large Language Models a Mirage?” arXiv [Cs.AI]. arXiv. http://arxiv.org/abs/2304.15004.

[^3]: I highly recommend [Ghostty](https://ghostty.org/) as a terminal. It is such a delightful work by [Mitchell Hashimoto](https://x.com/mitchellh). Though I have seen a trend to build complex UI interfaces through the CLI using frameworks like TUI and [Ink](https://github.com/vadimdemedes/ink). I personally worry that this could lead to anti-patterns where we are recreating the same complexity of traditional UIs in a CLI context.

[^4]: _Hill, Robert G., Jr, Lynn Marie Sears, and Scott W. Melanson._ 2013. “4000 Clicks: A Productivity Analysis of Electronic Medical Records in a Community Hospital ED.” The American Journal of Emergency Medicine 31 (11): 1591-94. 

[^5]: Important to notice how sudden the shift in our view of what a good user experience looks like. Just a year ago, building a web app with a sophisticated and well-crafted UI was considered innovative. Consider Linear, Figma, Vercel, and others. Now, it feels outdated and mostly an anti-pattern compared to agent-first systems.

Agent-first systems challenge this order; and the shift from sophisticated[^6] interfaces towards simpler interfaces is a premise to a more fundamental shift in how to build software. Taking the example of IDEs, it is certainly attempting to adopt LLMs and AI-driven programming by keeping the human in control of the code and having the LLMs as a side bar assistant. While it is a logical step, I share the same opinion as [Andrej Karpathy](https://x.com/karpathy)'s recent [tweet](https://x.com/karpathy/status/2004607146781278521) about how the struggle to increase productivity with LLMs is perhaps more of a human skill issue. I will also add that it is a tools issue and that we should not try to enforce LLMs into our routine tools but instead adopt the tools that are natural to an LLM to operate under.

[^6]:By sophisticated here I do not mean elegant, minimal, or refined in the sense of simplicity. I mean sophisticated in the more crude sense of feature-dense, highly parameterized systems that attempt to bring complexity directly to the user.

But this reconfiguration of human and machine roles does not arrive without friction. To move toward agent-first systems, we must confront a new class of questions that sit uncomfortably between engineering and philosophy. The challenge with the new paradigm is not simply enabling agents to do more, but ensuring that their expanding scope of action remains aligned with human values. Because LLMs are optimized for next-token prediction rather than grounded agency, alignment cannot be guaranteed by training alone. They are unlikely to be trusted in their current architecture to perform complex, critical tasks, such as treating a patient. Hence, additional layers of techniques and tools must be enforced — reinforcement learning, interfaces, constraints, oversight, and environment design. Unsolved, the cost of the opportunity and risk will remain significant. This is exemplified by the current gap between the excitement around the most recent LLMs and the fact that “95% of organizations are getting zero return” from AI initiatives.[^8] As long as we don't have an answer to those questions, the new wave of agent-first systems difficult to implement at scale. 

[^8]: *Nanda, M. I. T., Aditya Challapally, Chris Pease, Ramesh Raskar, and Pradyumna Chari.* 2025. “The Genai Divide State of Ai in Business 2025.” https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf.

Again, I don't think that it is solely the responsibility of LLMs and frontier model companies such as OpenAI and Anthropic to solve those challenges. Instead, these constraints suggest that the limiting factor for deploying agent-first systems is becoming less about the model intelligence (if you are a skeptic, performance) alone. What determines whether an LLM can act safely and usefully is increasingly the environment in which it operates. The problem of alignment is going to require at least for the foreseeable future *integrating* LLMs within an environment that allows them to operate safely—a *harness* that constrains, guides, and supervises a language model's behavior, rather than expecting those properties to be embedded in the model itself. The harness is the *environment around the model* that makes it usable, safe, and reliable in practice. The combination of the harness and LLM is what creates an agent-first system. We see this wave of environments, interfaces, and tools such as [Model-Context-Protocol](https://modelcontextprotocol.io/docs/getting-started/intro) (MCP), [Agent Skills](https://agentskills.io/home), and [Spec-driven Development](https://blog.kilo.ai/p/spec-driven-development-what-it-is) emerging in the last few months. The role of the harness is to ensure systematically and reliably that the LLM is operating within safe bounds. 

The corollary of this is that any given task where its outcomes cannot be reliably verified remains elusive for current LLMs at best and dangerous to deploy LLMs for at worst.[^9] For instance, healthcare operates under uncertainty, delayed outcomes, and asymmetric risk. Yet it is precisely this complexity that makes healthcare so compelling: the cost of human cognitive overload is real, and the promise of delegation is enormous. The limiting factor is not that agents lack the capacity to reason, but that the environments in which they would need to operate remain structurally unprepared for safe autonomy. In this context, many current responses by healthcare leaders in the AI space are directionally understandable but strategically insufficient. The proliferation of [AI governance playbooks](https://www.ihi.org/library/blog/ai-governance-maximizing-benefit-and-minimizing-harm-patients-providers-and-health), [health care AI model cards](https://www.chai.org/), and institutional AI policies — while well-intentioned — risk deflecting attention from the harder, more consequential work. They emphasize oversight by committee and static documentation over the design of operational environments in which autonomy can be exercised, constrained, and audited in practice. Without addressing this foundational layer, governance efforts may provide the appearance of control while doing little to advance the safe and effective deployment of agent-first systems.

[^9]: _Wen, Xumeng, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, et al._ 2025. “Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs.” arXiv [Cs.AI]. arXiv. http://arxiv.org/abs/2506.14245.

A common — and understandable though naive — approach is to deploy agents only within tightly scoped environments with _human-in-the-loop_ able to verify or intervene at every step. This reflects a defensive posture shaped by legitimate concerns around safety, risk, and accountability. However, as AI capabilities continue to advance, the limiting factor will increasingly be neither the model nor the task, but the human oversight or the constraints of the environment itself. Crucially, this is not an argument against human involvement in ensuring safety. Rather, it challenges a deeper and often implicit assumption that continuous human intervention can serve as a reliable, scalable mechanism for safety in agent-first systems. In practice, this assumption breaks down at scale. A case in point is the [recent Waymo incident](https://www.theatlantic.com/technology/2025/12/waymo-robotaxi-san-francisco-blackout/685393/). When an outage occurred in San Francisco, multiple Waymo cars stalled and became inoperable, blocking roads and intersections. Waymo had to cease operations for several hours. The failure, as it was revealed, was not in the AI engine, but in Waymo's control system. Waymo's system relied on a [human-in-the-loop fleet response](https://waymo.com/blog/2024/05/fleet-response) as part of its safety harness — a solution that appeared robust until it was not under the cheer weight of scale. This episode underscores that in critical systems, human-in-the-loop is not a viable control strategy. It collapses precisely when scale, speed, and coordination matter most. It merely shifts risk from model error to coordination failure between machines and humans.[^10] The same pattern will repeat across domains unless we move beyond human-in-the-loop as a safety crutch and toward environments—and harnesses—that make autonomy governable by design.

[^10]: _Chiodo, Maurice, Dennis Müller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, and John Burden._ 2025. “Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility.” arXiv [Cs.CY]. arXiv. https://doi.org/10.48550/arXiv.2505.10426.

The transition to agent-first systems is as much a question of design philosophy as technical capability. What is at stake is not whether agents can reason, but whether we can build environments in which their reasoning-how primitive that is-operates safely, transparently, and in service of human intent. This requires moving beyond the reflex to keep humans in every loop and toward the harder work of designing systems where autonomy is possible. The human role shifts from _producer of knowledge_ to _supervisor of outcomes_.[^11] The future of software lies not in interfaces that guide human thought, but in agents that perform tasks based on human objectives. The challenge before us is to build the harnesses that make this new paradigm safe, reliable, and aligned with our values. If (when) successful, what emerges instead is a model in which knowledge production is increasingly delegated and knowledge becomes more abundant. The next generation of software will be built around agents, not user interfaces.

[^11]: I attended a health care-focus AI round-table with Greg Beckman from OpenAI where he stated that “we should think about our role when the cost of knowledge gets to zero with LLMs”

For builders, I offer the following simple litmus test: Remove the agent from your system. If the product collapses, you are on the right track - but it is not guaranteed that you building an agent-first system. If it does not though, AI remains a feature rather than a foundation and you are certainly not building an agent-first system.