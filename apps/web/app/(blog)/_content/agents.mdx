---
title: Agent-First Systems
subtitle: Redefining Software in the Age of AI Agents
featured: true
publishedAt: 2025-12-27
category: engineering
---

import { Callout } from "@workspace/ui/blocks/callout";
import { Quote } from "@workspace/ui/blocks/quote";
import { ThemeImage } from "@workspace/ui/blocks/themed-image";

<Callout type="info">
Over the holidays, I built an AI Assistant using Cloudflare Durable Objects. The source code is available [https://github.com/meleksomai/os](https://github.com/meleksomai/os) and you can try it by sending an email to [hello@somai.me](mailto:hello@somai.me). Building on that experience, this essay explores how agent-first systems are not merely a new feature layer atop existing tools, but a complete shift in the way we design and interact with systems.
</Callout>

Over the past few months, and more precisely with the release of [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) and [Gemini 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro), I have become increasingly convinced that the future of software lies in *agent-first* systems.[^1] By systems, I mean software in which the task to be performed *by* an end-user is further removed *from* the end-user. As current AI agents empowered by LLMs are increasingly capable of long-context reasoning, tool use, and multi-turn interaction, the tasks they can handle are becoming progressively _(and then suddenly)_ the property of the agent. Agents manage the intent interpretation, memory, decision-making, action execution, and ultimately *removes* further the user from the task itself. The shift could be thought of as analogous to the way modern high-level programming languages such as Python, JavaScript, and others abstract away garbage collection, memory allocation, etc. However, the similarity ends here.

[^1]: This is making the assumption that Frontier LLMs will continue to improve in capability, reliability, and safety over time; and that whether we achieve AGI or not, these models will be powerful enough to handle a wide range of tasks that we currently rely on traditional software for.

This paradigm shift is revealed through the interfaces we as human interact with machines. The current interfaces of modern software—from spreadsheets to dashboards to EHRs—keep humans as the primary agents of understanding and action, with machines serving as fast, precise, but ultimately *inert* objects. In almost any software we use, the user has to navigate menus, click on buttons, and fill out forms. Those interfaces are often complex, cumbersome, and frustrating to use to a point where professionals, such as ED physicians spend significantly more time entering data into those systems than on any other activity, including direct patient care.[^2]

[^2]: _Hill, Robert G., Jr, Lynn Marie Sears, and Scott W. Melanson._ 2013. “4000 Clicks: A Productivity Analysis of Electronic Medical Records in a Community Hospital ED.” The American Journal of Emergency Medicine 31 (11): 1591-94.

Software designers have done their best to create incredible [design](https://www.radix-ui.com) [systems](https://mui.com/material-ui/) to make the user experience less frustrating. Engineering writers will create lengthy training materials to onboard new users, and credentials are now bestowed upon those who achieve mastery of a tool itself rather than the task to be achieved.[^3] The tools are perfected to guide and structure human reasoning, not replace it since the thought was that machine can't “reason”. 

[^3]: I am amazed how sudden the shift in our view of what a good software looks like. Just a year ago, building a web app with a sophisticated and well-crafted UI was considered innovative. Consider Linear, Figma, Vercel, and others. Now, it feels outdated and mostly an anti-pattern compared to agent-first systems.

_Agent-first systems challenge this hierarchy._ And hopefully, it will leapfrog human-machine interface.

The first inflection[^4] point toward agent-based software is without doubt [ChatGPT](https://chatgpt.com/). The conversational interface made it easy to express intent, iterate on ideas, and experience the capabilities of the LLM model. The second and more powerful _Aha!_ moment came more recently when I started using AI-assisted coding with [Codex](https://openai.com/codex/), [Claude Code](https://github.com/anthropics/claude-code), and [Amp](https://ampcode.com/). All these coding tools use a simple Command-line Interface (CLI).[^5] The simplicity of the CLI unlocks instinctively a more natural and frictionless way to bring LLMs to my day-to-day programming routine. Those CLI-native tools surpass the more complex IDE-based tools such as [VS Code Copilot](https://code.visualstudio.com/docs/copilot/overview), [Cursor](https://cursor.com/) or [Zed](https://zed.dev/) that I have tried in the past months. I genuinely feel that those tools hinder my ability to leverage LLMs properly. Those IDEs are certainly attempting to assist the current generation of programmers adopt LLMs and AI-driven programming by keeping the human in control of the code and having the LLMs as a side bar assistant. While it is a logical step, I share the same opinion as [Andrej Karpathy](https://x.com/karpathy)'s recent [tweet](https://x.com/karpathy/status/2004607146781278521) about how the struggle to increase productivity with LLMs is perhaps more of a human skill issue. I will also add that it is a tools issue and that we should not try to enforce LLMs into our routine tools but instead adopt the tools that are natural to an LLM to operate under.

[^4]: While I describe LLM progress in terms of a step function, the underlying improvements are smoother and predictable. What feels like a sudden capability jump is often a perceptual bias shaped by our limited human capacity to measure change over a continuous spectrum. I highly recommend the following paper: <br /> _Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo._ 2023. “Are Emergent Abilities of Large Language Models a Mirage?” arXiv [Cs.AI]. arXiv. http://arxiv.org/abs/2304.15004.

[^5]: I highly recommend [Ghostty](https://ghostty.org/) as a terminal. It is such a delightful work by [Mitchell Hashimoto](https://x.com/mitchellh). Though I have seen a trend to build complex UI interfaces through the CLI using frameworks like TUI and [Ink](https://github.com/vadimdemedes/ink). I personally worry that this could lead to anti-patterns where we are recreating the same complexity of traditional UIs in a CLI context.

As agents become more capable, the traditional role of interfaces—to guide human reasoning step by step—begins to erode. The human role shifts from _producer of knowledge_ to _supervisor of outcomes_.[^6] The value no longer lies in performing the reasoning step-by-step, but in setting direction, defining constraints, and judging results. This is not simply a matter of efficiency; it unsettles the core premise of that the knowledge economy is built around cognition as the scarce resource. What emerges instead is a model in which knowledge production is increasingly delegated and knowledge becomes more abundant.

[^6]: I attended a health care-focus AI roundtable with Greg Beckman from OpenAI where he stated that “we should think about our role when the cost of knowledge gets to zero with LLMs”

<Quote>
The next generation of software will be built around agents, not user interfaces.
</Quote>

But this reconfiguration of human and machine roles does not arrive without friction. To move toward agent-first systems, we must confront a new class of questions that sit uncomfortably between engineering and governance. The challenge is not simply enabling agents to do more, but ensuring that their expanding scope of action remains aligned with human values. Because LLMs are optimized for next-token prediction rather than grounded agency, alignment cannot be guaranteed by training alone; it must be enforced through external structure — reinforcement learning, interfaces, constraints, oversight, and environment design. They are unlikely to be trusted in their current architecture to perform complex, critical tasks, such as treating a patient. 

This makes the new wave of agent-first systems difficult to implement at scale as long as we don't have an answer to those questions. Again, I don't think that it is solely the responsibility of LLMs as thinking machines to solve these challenges for themselves. Instead, these constraints suggest that the limiting factor for deploying agent-first systems is becoming less about the model intelligence (if you are a skeptic, performance) alone. What determines whether an LLM can act safely and usefully is increasingly the environment in which it operates. Those constraints are going to become the bottleneck in AI deployment. Unsolved, the opportunity cost and risk will remain significant. This is exemplified by the current gap between the excitement around the most recent LLMs and the fact that “95% of organizations are getting zero return” from AI initiatives.[^7] That is the next bottleneck in AI scaling. 

[^7]: *Nanda, M. I. T., Aditya Challapally, Chris Pease, Ramesh Raskar, and Pradyumna Chari.* 2025. “The Genai Divide State of Ai in Business 2025.” https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf.

A common — and understandable — response is to deploy agents only within tightly scoped environments with humans able to verify or intervene at every step, _human-in-the-loop_. This reflects a defensive posture shaped by legitimate concerns around safety, risk, and accountability. However, this approach systematically underutilizes the very capabilities we are actively training into these systems. As AI capabilities continue to advance, the limiting factor will increasingly be neither the model nor the task, but the human oversight or the constraints of the environment itself.

Crucially, this is not an argument against human involvement in ensuring safety. Rather, it challenges a deeper and often implicit assumption that continuous human intervention can serve as a reliable, scalable compensatory mechanism for failures in LLM-driven systems. In practice, this assumption breaks down at scale.

A case in point is the [recent Waymo incident](https://www.theatlantic.com/technology/2025/12/waymo-robotaxi-san-francisco-blackout/685393/). When an outage occurred in San Francisco, multiple Waymo cars stalled and became inoperable, blocking roads and intersections. Waymo had to cease operations for several hours. The failure, as it was revealed, was not in the AI engine, but in Waymo's control system. Waymo's system relied on a [human-in-the-loop fleet response](https://waymo.com/blog/2024/05/fleet-response) as part of its safety harness — a solution that appeared robust on paper but proved brittle in practice. This episode underscores that in critical systems, human-in-the-loop is not a viable control strategy. It collapses precisely when scale, speed, and coordination matter most. It merely shifts risk from model error to coordination failure between machines, humans, and infrastructure.[^8] The same pattern will repeat across domains unless we move beyond human-in-the-loop as a safety crutch and toward environments—and harnesses—that make autonomy governable by design.

[^8]: _Chiodo, Maurice, Dennis Müller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, and John Burden._ 2025. “Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility.” arXiv [Cs.CY]. arXiv. https://doi.org/10.48550/arXiv.2505.10426.

The better answer is that the problem of alignment is going to require at least for the foreseeable future, the problem of alignment will require *integrating* LLMs within an environment that allows them to operate safely—a *harness* that constrains, guides, and supervises a language model's behavior, rather than expecting those properties to be embedded in the model itself. The harness is the *environment around the model* that makes it usable, safe, and reliable in practice. The combination of the harness and LLM is what creates an agent-first system. We see this wave of environments, interfaces, and tools such as [Model-Context-Protocol](https://modelcontextprotocol.io/docs/getting-started/intro) (MCP), [Agent Skills](https://agentskills.io/home), and [Spec-driven Development](https://blog.kilo.ai/p/spec-driven-development-what-it-is) emerging in the last few months. The role of the harness is to ensure systematically and reliably that the LLM is operating within safe bounds. 

The corollary of this is that any given task where its outcomes cannot be reliably verified remains elusive for current LLMs at best and dangerous to deploy LLMs for at worst.[^9] There is obviously a long way to go to reach that across industries. For instance, healthcare sits at the sharp edge of this tension. Unlike domains where success can be measured through immediate, binary feedback, healthcare operates under uncertainty, delayed outcomes, and asymmetric risk. Decisions unfold across time, institutions, and professional boundaries, and verification is rarely straightforward. Yet it is precisely this complexity that makes healthcare so compelling: the cost of human cognitive overload is real, and the promise of delegation is enormous. The limiting factor is not that agents lack the capacity to reason, but that the environments in which they would need to operate remain structurally unprepared for safe autonomy. In this context, many current responses by healthcare leaders in the AI space are directionally understandable but strategically insufficient. The proliferation of [AI governance playbooks](https://www.ihi.org/library/blog/ai-governance-maximizing-benefit-and-minimizing-harm-patients-providers-and-health), [health care AI model cards](https://www.chai.org/), and institutional AI policies — while well-intentioned — risk deflecting attention from the harder, more consequential work. They emphasize oversight by committee and static documentation over the design of operational environments in which autonomy can be exercised, constrained, and audited in practice. Without addressing this foundational layer, governance efforts may provide the appearance of control while doing little to advance the safe and effective deployment of agent-first systems.

[^9]: _Wen, Xumeng, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, et al._ 2025. “Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs.” arXiv [Cs.AI]. arXiv. http://arxiv.org/abs/2506.14245.

We are living through an intermediate phase in which most software is still designed according to pre-agent assumptions, even as the systems we build increasingly depend on LLMs and autonomous agents at their core. Incumbent platforms, in particular, have responded by layering AI onto existing products—code completion inside IDEs, chat interfaces bolted onto CRMs, assistants embedded within workflows that remain fundamentally human-driven. This approach is understandable, but it is unlikely to endure. In practice, retrofitting agents into software designed for deterministic tools produces interfaces that are awkward for machines and increasingly incoherent for humans. As agent capabilities expand, this tension will become unsustainable. The result is often the worst of both worlds: brittle autonomy constrained by legacy workflows, and user experiences that degrade as systems oscillate between automation and manual control. Over time, such designs become impractical — not because agents are insufficiently capable, but because the surrounding software was never meant to accommodate them.

For builders, I offer the following simple litmus test. Remove the agent from your system. If the product collapses, you are building an agent-first system. If it does not, AI remains a feature rather than a foundation. This distinction matters, because agent-first systems demand fundamentally different assumptions about interfaces, control, verification, and responsibility.

This reconfiguration helps explain both the excitement and the unease surrounding AI systems. It is not just jobs that feel threatened, but an entire self-conception: the idea that thinking is what we do, and tools merely help us do it better.The question ahead is not whether this transition will occur, but whether we are willing to redesign our software to meet it with clarity rather than reluctance.