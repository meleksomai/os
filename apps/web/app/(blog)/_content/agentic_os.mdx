---
title: Building an Email AI-Assistant
subtitle: Productivity with LLMs, Cloudflare Workers, and Durable Objects
featured: true
publishedAt: 2025-12-27
category: engineering
---

import { Callout } from "@workspace/ui/blocks/callout";
import { Quote } from "@workspace/ui/blocks/quote";
import { ThemeImage } from "@workspace/ui/blocks/themed-image";

<Callout type="info">
The source code for this project is available on [https://github.com/meleksomai/os](https://github.com/meleksomai/os).
</Callout>

Over the past year, I have become more and more convinced that the future of softwares lies in _agent-first_ systems.[^1] By agent-first systems, I mean software systems in which the task to be performed _by_ an end-user is further removed _from_ the end-user. The task is the entire property of the agent, which manages the intent interpretation, memory, decision-making, action execution, and ultimately _removes_ further the user from the task itself. This is analogous - _to an extend_ - to the way modern high-level programming languages such as Python, Javascript, and others abstract away garbage collection, memory allocation, etc. However, the similarities ends here. LLMs pushes the boundary to perform more tasks that involve "thinking", "conceptualization", and ultimately "craft". The downside is that we are moving towards a technology landscape where the end user has less control over the task execution. The upside is that it can unlock a new level of productivity and creativity by delegating handling complex tasks to LLMs. 

[^1]: This is making the assumption that Frontier LLMs will continue to improve in capability, reliability, and safety over time; and that whether we achieve AGI or not, these models will be powerful enough to handle a wide range of tasks that we currently rely on traditional softwares for.

This shifting of task execution to agents creates a new accountability to the end-user whom responsibility becomes more about defining the _what_ rather than the _how_ of a task. This is tricky since it requires clear goals and constraints that are sufficient, finite, and verifiable[^2].

[^2]: Verifiability is an extremely powerful concept that has emerged recently in the LLM space. At the moment, there still open questions but with the current LLM capabilities, verifiability is a requirement to build reliable AI systems. Any domain that cannot be verified is likely not suitable for LLMs at the moment. Health care is a prime example of this challenge.

Fundamentally, a different user interaction (UX/UI) from traditional software seems to emerge from this shift. In _traditional_[^3] softwares the user is expected to interact with a specific UI to accomplish a task. They have to navigate menus, click on buttons, and fill out forms. Software designers would create incredible [design](https://www.radix-ui.com) [systems](https://mui.com/material-ui/) to make the user experience less frustrating, training materials have to be produced to train new users on how to use the software, and credentials are now bestowed upon those who achieve mastery of a tool itself rather than the task to be achieved. With LLMs on the other hand, the user no longer needs to learn how to use a piece of software; instead, they can rely on the agent to understand their intent and use the appropriate tools to achieve their goals. 

[^3]: I am amazed how sudden the shift in our view of what a good software looks like. Just a year ago, building a web app with a sophisticated and well-crafted UI was considered innovative. Consider Linear, Figma, Vercel, and others. Now, it feels outdated and mostly an anti-pattern compared to agent-first systems.

How far are we from this future? I think we are closer than we think. We are in this transition phase where we are still building traditional softwares, but increasingly more systems are being built around LLMs and agents. Most incumbent softwares are trying to integrate LLMs as a feature (e.g., code completion in IDEs, chatbots in CRMs, etc). However, this lead in my observation to two things: 1) worst UI experience, and 2) unpractical AI tool. If you are a software company, a litmus test is to try removing the agent from your software. If your software collapses, then yes, it's agent-first. If not, AI is just a feature.

There is obviously a long way to go to reach that across industries, such as health care. However, we are already seeing a glimpse of this future and it is exciting.

## From Software to Agents

The first _inflection_[^3] point towards this direction of agent-based software is without doubt [ChatGPT](https://chatgpt.com/). Its simple chat interface unlocked a new way to interact with intelligent systems that felt natural, and powerful. The conversational model made it easy to express intent, iterate on ideas, and access the capabilities of the LLM model. I think OpenAI and Sam Altman brilliant move was to realize that the best interface to let people experience ChatGPT was to build a _UI-less_ interface.

<Quote>
The next generation of software will be built around agents, not user interfaces.
</Quote>

[^3]: While I describe LLM progress in terms of a step function, the underlying improvements are smoother and predictable. What feels like a sudden capability jump is often a perceptual bias shaped by our limited human capacity to measure change over a continuous spectrum. I highly recommend the following paper: <br /> _Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo._ 2023. “Are Emergent Abilities of Large Language Models a Mirage?” arXiv [Cs.AI]. arXiv. http://arxiv.org/abs/2304.15004.

The second and more powerful _Aha!_ moment came more recently when I started using (vibe) coding with [Codex](https://openai.com/codex/), [Claude Code](https://github.com/anthropics/claude-code), and [Amp](https://ampcode.com/). All these vibe coding tools used yet another simple Command-line Interface (CLI).[^4] The simplicity[^5] of the CLI unlocks instinctively a more natural and frictionless way to bring LLMs to my day-to-day programming routine. I use VSCode Copilot, and Cursor but, I feel that those tools regress my ability to leverage LLMs. Cursor and VS Code are for sure attempting to _assist_ the current generation _adopt_ LLMs. In my opinion, while it has some value, it is counterproductive. I have the same opinion than [Andrej Karpathy](https://x.com/karpathy)'s recent [tweet](https://x.com/karpathy/status/2004607146781278521) about how the struggle to increase productivity with LLMs is perhaps more of a skill issue, _and also tools issue_.

[^4]: I highly recommend [Ghostty](https://ghostty.org/) as a terminal. It is such a delightful work by [Mitchell Hashimoto](https://x.com/mitchellh).
[^5]: Though I have seen a trend to build complex UI interfaces through the CLI using frameworks like TUI and [Ink](https://github.com/vadimdemedes/ink). I personally worry that this could lead to anti-patterns where we are recreating the same complexity of traditional UIs in a CLI context.

This trend - or perhaps principle - of _removing the UI_ unlock LLMs as a new interaction paradigm that requires rethinking how we should build software from the ground up.

## Applying the Principle to my Email Inbox

So during the holidays, aside from reading the latest issue of [London Review of Books](https://www.lrb.co.uk/), I got sometime to ponder of the idea of UI-less computing and what it would look like to build softwares around LLMs from first-principles. I wanted to leverage a simple interface. It happens that during the same holidays, I had to deal with a backlog of emails (sorry if I did not reply to any of those. Now you know why). Emails after all are one of the most universal protocols for communication.

What if I can build an agent that can help me triage, prioritize, and respond to emails on my behalf? More interestingly, what if we can have historical context per email contact (an individual, a company, etc) so the LLM can adjust its behavior for each contact.  What if I can build an agent that can remember the context of my previous conversations and help me draft replies that are consistent with my tone and style? What if I can build an agent that can learn from my feedback and improve over time? And more importantly, what if I can build this agent without a custom UI, but rather using email as the interface?

<Callout>
You can skip the more technical piece below where I will share the high-level architecture, design decisions, and challenges I faced while building this agentic OS. You can check out the source code for this project on [GitHub](https://github.com/meleksomai/os); or you can try it ut by sending me an email at [hello@somai.me](mailto:hello@somai.me).
</Callout>

## Platform: Cloudflare Workers + Durable Objects

As I said, I wanted to build this system from first principles. I wanted to avoid building a web app or a mobile app first. I wanted to start with the simplest possible architecture that can scale over time. Over the last few years, I have become a big fan of [serverless platforms](youtube.com/watch?v=mlP_oP5LrIQ&themeRefresh=1). So, I was sure that I needed a simple, serverless architecture to start with. [Vercel](https://vercel.com/) is amazing for web apps and their recent [fluid compute](https://vercel.com/fluid), [AI SDK](https://ai-sdk.dev/), and [Workflow](https://vercel.com/docs/workflow) were intriguing enough. However, Vercel defaults to be tightly coupled to [Next.js](https://nextjs.org/) and the web framework lifecycle. That felt unnatural for an agent-first system. I really like Next.js and love the craft the team at Vercel put into building all the products and services, but I did not want to build a web app first and start from first principles. [AWS](https://aws.amazon.com/) is powerful, but is becoming a very difficult platform to start with. I did not want to setup VPCs, configure Bedrock, manage IAM, connect my CI with Cloudformation and orchestrate a mountain of infrastructure pieces just to get a prototype running. Also, I really find the development experience on AWS Lambda to be quite challenging compared to Vercel.

So, I landed on [Cloudflare Workers](https://workers.cloudflare.com/). Cloudflare felt like the middle ground: AWS-level primitives with Vercel-like developer experience. I had the least experience with Cloudflare compared to Vercel or AWS, but it is the most balanced starting point I have found. I am also a big fan of Cloudflare's philosophy around edge computing, and their (somewhat) recent innovation around [durable objects](https://developers.cloudflare.com/durable-objects/).

_Note: I did not have much experience with Cloudflare before starting this project; and I was able to get a prototype running and deployed the full platform in a few hours. The development experience was smooth, specifically thanks to really good local dev experience with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/). The documentation is _okay_ overall, though it is confusing at times._

But the most important aspect of Cloudflare that caught my attention and was definitely the ultimate reason to choose the platform was their recent push for [Durable Objects](https://developers.cloudflare.com/durable-objects/) (DOs). I personally think that DOs is a game-changer. It is perhaps the coolest thing I have seen in serverless computing in the last few years since AWS Lambda and DynamoDB. The concept is pretty simple. Rathe than thinking about scale in terms of distributed systems, DOs let you think about scale in terms of _stateful small machines/instances_. Each instance is a self-contained unit of compute with its own memory, storage, and lifecycle. It is Lambda on steroids. Cloudflare did a really good job of combining the _serverless_ model with the _stateful_ model. This is a perfect match for building agent-first systems where each agent can be a DO instance with its own state and behavior. DOs also adds concepts such as _scheduling_ that enables a DOs to have its own lifecycle.[^6]

[^6]: I am excited but also skeptical. DOs seems to be too good to be true. I am curious to see how Cloudflare will execute on this vision over time. There are still many open questions around DOs, and I have yet to see successful businesses and ideas built on top.

## Architecture

Now, that we have chosen our platform. It is time to start building. The architecture I ended up using is fairly simple: an email is received by Cloudflare Email Routing, forwarded to a Cloudflare Worker that routes the email to a specific agent (Durable Object) instance based on the sender's email address. The agent processes the email, runs LLMs, updates its state, and optionally sends a reply.

<ThemeImage
  lightSrc="/images/essays/cloudflare_email_architecture_light.png"
  darkSrc="/images/essays/cloudflare_email_architecture_dark.png"
  alt="Architecture diagram"
  width={1200}
  height={489}
/>

I will focus on the most interesting parts of the architecture: 1) _routing_ logic, 2) AI _memory_, 2) AI Assistant _lifecycle_, and 3) some of the _missing_ pieces.

### Routing

Since I am using Durable Objects, an incoming email must be _routed_ to its corresponding agent instance. The critical workflow I wanted to implement is to be able to send "confidential" emails to the agent to remember some context about a specific contact. Again, my goal is to rely on the email for _all_ interactions. I want to send an email to my agent "You can share my phone with Alice." The agent should then store this context in its memory and use it to inform future interactions with Alice. Thus, the routing logic must be able to distinguish between emails sent by me (the owner of the agent) and emails sent by other contacts. 

Each email has a set of headers that can be used to identify the thread. The most relevant headers are `Message-ID`, `In-Reply-To`, and `References`. These headers are used by email clients (e.g., Gmail, Outlook, etc) to group emails into threads. So, if I reply to an email from Alice that only includes my AI Assistant email, the email will have the same `In-Reply-To` and `References` headers as the original email from Alice. This allows me to identify the thread and route the email to the correct agent instance.

I used a simple KV store to map thread identifiers to agent instance IDs. When an email is received, the routing logic checks the headers for thread identifiers. If a match is found in the KV store, the email is routed to the corresponding agent instance. If no match is found, a new agent instance is created, and the thread identifier is stored in the KV store.

```typescript title="resolver.ts" {4,10-12}
export function createThreadBasedEmailResolver<Env>(
  agentName: string,
  ownerEmail: string,
  store: KVNamespace
): EmailResolver<Env> {
  return async (email: ForwardableEmailMessage, _env: Env) => {
    // ...

    // Extract thread identifiers from headers
    const messageId = email.headers.get("Message-ID");
    const inReplyTo = email.headers.get("In-Reply-To");
    const references = email.headers.get("References");

    // ...

    // Store the mapping for future lookups
    await store.put(threadId, contact, {
      expirationTtl: 60 * 60 * 24 * 90,
    });

    return {
      agentName,
      agentId: contact,
    };
  };
}
```

Ok, now that we have routing, let's talk about memory.

### Memory

We now have routed our email to the correct agent instance. The next step is to make the instance remember the context of the conversation. As mentioned earlier, each DOs has its own memory, a simple SQLLite database. This architecture makes the memory management straightforward. Each agent instance can store its own state in its own database. The memory schema is simple:

```typescript {3} title="memory.ts" 
export type Memory = {
  lastUpdated: Date | null;
  messages: Message[];
  context: string;
  summary: string;
  hasAutoReplied: boolean;
};
```

### AI Assistant Lifecycle

The AI Assistant lifecycle is divided into three main phases: 1) _ingestion_, 2) _processing_, and 3) _action_.

## Try it out

Send me an email at `hello@somai.me` and you will be routed to the agent. It is early, but it already works.

If you want to follow the build, the code lives in `apps/agent` and the public surface lives in `apps/web`.
